{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks6Z-cXYlPao",
        "outputId": "3ffdb746-48a7-444c-8790-e99acead3332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-18 15:16:27--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.182.103.9, 52.217.171.177, 52.216.61.249, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.182.103.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip       100%[===================>]  18.01G  17.2MB/s    in 18m 0s  \n",
            "\n",
            "2024-05-18 15:34:27 (17.1 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2024-05-18 15:34:27--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.218, 52.217.107.36, 3.5.25.102, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.218|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  17.7MB/s    in 49s     \n",
            "\n",
            "2024-05-18 15:35:17 (16.0 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2024-05-18 15:35:17--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.122.137, 3.5.8.191, 3.5.29.136, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.122.137|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  9.68MB/s    in 26s     \n",
            "\n",
            "2024-05-18 15:35:43 (9.12 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download COCO 2017 train images\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "# Download COCO 2017 val images\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "# Download COCO 2017 annotations\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vHLJFWcldoK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERiykwRslfSK"
      },
      "outputs": [],
      "source": [
        "# Function to unzip files\n",
        "def unzip_file(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2l1dFtYlgW_"
      },
      "outputs": [],
      "source": [
        "# Unzip train images\n",
        "unzip_file('train2017.zip', '/content/coco')\n",
        "# Unzip val images\n",
        "unzip_file('val2017.zip', '/content/coco')\n",
        "# Unzip annotations\n",
        "unzip_file('annotations_trainval2017.zip', '/content/coco')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ba9UGnlhYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405245b2-1af2-4d3a-f518-20fcc850ff07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "000000000009.jpg\n",
            "000000000025.jpg\n",
            "000000000030.jpg\n",
            "000000000034.jpg\n",
            "000000000036.jpg\n",
            "000000000139.jpg\n",
            "000000000285.jpg\n",
            "000000000632.jpg\n",
            "000000000724.jpg\n",
            "000000000776.jpg\n",
            "captions_train2017.json  instances_train2017.json  person_keypoints_train2017.json\n",
            "captions_val2017.json\t instances_val2017.json    person_keypoints_val2017.json\n"
          ]
        }
      ],
      "source": [
        "# Verify contents\n",
        "!ls /content/coco/train2017 | head -n 5\n",
        "!ls /content/coco/val2017 | head -n 5\n",
        "!ls /content/coco/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycXrhzuOlid2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dd99df-3f6e-45d5-e2d7-cc8a6cd219e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the folder: 118287\n",
            "Number of files in the folder: 5000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def numfiles(folder_path):\n",
        "  files = os.listdir(folder_path)\n",
        "  num_files = len(files)\n",
        "  return num_files\n",
        "\n",
        "# Specify the path to the folder in your Google Drive\n",
        "\n",
        "folder_path_train = '/content/coco/train2017'\n",
        "folder_path_val = '/content/coco/val2017'\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of files in the folder:\", numfiles(folder_path_train))\n",
        "print(\"Number of files in the folder:\", numfiles(folder_path_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TF_ENABLE_ONEDNN_OPTS=0"
      ],
      "metadata": {
        "id": "Tw2-UMsuFD70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from pycocotools.coco import COCO\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CocoCaptions\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5NjxBCMXFGmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class COCODataset(Dataset):\n",
        "    def __init__(self, data_dir, data_type, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.data_type = data_type\n",
        "        self.transform = transform\n",
        "        self.img_ids, self.captions = self.load_images_and_captions()\n",
        "\n",
        "    def load_images_and_captions(self):\n",
        "        # Load captions\n",
        "        captions_path = os.path.join(self.data_dir, 'annotations', f'captions_{self.data_type}.json')\n",
        "        with open(captions_path, 'r') as f:\n",
        "            captions_data = json.load(f)\n",
        "        captions = [caption['caption'] for caption in captions_data['annotations']]\n",
        "\n",
        "        # Load image IDs\n",
        "        img_ids = [img_info['id'] for img_info in captions_data['images']]\n",
        "\n",
        "        return img_ids, captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        image_path = os.path.join(self.data_dir, self.data_type, f'{img_id:012d}.jpg')\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        caption = self.captions[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, caption"
      ],
      "metadata": {
        "id": "6vmIXFsTFIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "PTOGKNT2FJVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_dir = '/content/coco'  # Update this path\n",
        "data_type = 'train2017'\n",
        "dataset = COCODataset(data_dir, data_type, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
      ],
      "metadata": {
        "id": "m3T7CoJZFUyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
        "        outputs = self.bert(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(768, 256*16*16)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, text_features):\n",
        "        x = self.fc(text_features)\n",
        "        x = x.view(-1, 256, 16, 16)\n",
        "        x = self.deconv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "F-7wN-WvFVlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 1, 4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return torch.sigmoid(x.view(x.size(0), -1)).mean(1, keepdim=True)  # Ensure the output size is [batch_size, 1]\n"
      ],
      "metadata": {
        "id": "qUBkfYM6FXU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "text_encoder = TextEncoder().cuda()\n",
        "generator = Generator().cuda()\n",
        "discriminator = Discriminator().cuda()"
      ],
      "metadata": {
        "id": "Gk4LOvrJFaY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss functions and optimizers\n",
        "adversarial_loss = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "L393cRQFFb3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_texts = [\n",
        "    \"A cat sitting on a bench\",\n",
        "    \"A beautiful landscape with mountains\",\n",
        "    \"A group of people playing football\",\n",
        "    \"A close-up of a colorful bird\",\n",
        "    \"A city skyline at night\"\n",
        "]"
      ],
      "metadata": {
        "id": "RqG4D6GRK2Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_pipeline(text_prompts,epoch):\n",
        "    output_dir = \"/content/coco/generated_outs\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    text_encoder.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i,text in enumerate(text_prompts):\n",
        "            text_features = text_encoder([text]).cuda()\n",
        "            generated_image = generator(text_features)\n",
        "            generated_image = generated_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "            generated_image = (generated_image + 1) / 2  # Denormalize\n",
        "\n",
        "            plt.imshow(generated_image)\n",
        "            plt.axis('off')\n",
        "            plt.savefig(os.path.join(output_dir, f'epoch_{epoch}_image_{i}.png'))\n",
        "            plt.close()\n",
        "\n",
        "    return generated_image"
      ],
      "metadata": {
        "id": "E9kWvMl4K382"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs, dataloader):\n",
        "    text_encoder.train()\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    train_losses_G = []\n",
        "    train_losses_D = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_G = 0\n",
        "        epoch_loss_D = 0\n",
        "\n",
        "        for i, (images, captions) in enumerate(dataloader):\n",
        "            batch_size = images.size(0)\n",
        "            images = images.cuda()\n",
        "            valid = torch.ones(batch_size, 1).cuda()\n",
        "            fake = torch.zeros(batch_size, 1).cuda()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            text_features = text_encoder(captions).cuda()\n",
        "            generated_images = generator(text_features)\n",
        "\n",
        "            g_loss = adversarial_loss(discriminator(generated_images), valid)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            real_loss = adversarial_loss(discriminator(images), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            epoch_loss_G += g_loss.item()\n",
        "            epoch_loss_D += d_loss.item()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], G Loss: {g_loss.item():.4f}, D Loss: {d_loss.item():.4f}')\n",
        "\n",
        "        epoch_loss_G /= len(dataloader)\n",
        "        epoch_loss_D /= len(dataloader)\n",
        "        train_losses_G.append(epoch_loss_G)\n",
        "        train_losses_D.append(epoch_loss_D)\n",
        "\n",
        "        # Logging the epoch losses\n",
        "        writer.add_scalar('Loss/Generator', epoch_loss_G, epoch)\n",
        "        writer.add_scalar('Loss/Discriminator', epoch_loss_D, epoch)\n",
        "\n",
        "        # Save model checkpoints\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'text_encoder_state_dict': text_encoder.state_dict(),\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "            'loss_G': epoch_loss_G,\n",
        "            'loss_D': epoch_loss_D,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "        inference_pipeline(eval_texts,epoch)\n",
        "\n",
        "    return train_losses_G, train_losses_D\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_losses_G, train_losses_D = train_model(epochs=50, dataloader=dataloader)\n",
        "\n",
        "# Close the writer\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "Mj7RirDrFdcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7111c469-8029-45aa-e67b-c244dfc6bdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [1/232], G Loss: 0.7063, D Loss: 0.6684\n",
            "Epoch [1/50], Step [101/232], G Loss: 4.0336, D Loss: 0.0205\n",
            "Epoch [1/50], Step [201/232], G Loss: 3.6305, D Loss: 0.4282\n",
            "Epoch [2/50], Step [1/232], G Loss: 3.4248, D Loss: 0.4274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_display(text):\n",
        "    text_encoder.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = text_encoder([text]).cuda()\n",
        "        generated_image = generator(text_features)\n",
        "\n",
        "    generated_image = generated_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "    generated_image = (generated_image + 1) / 2  # Denormalize\n",
        "    plt.imshow(generated_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(train_losses_G, train_losses_D):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses_G, label='Generator Loss')\n",
        "    plt.plot(train_losses_D, label='Discriminator Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "generate_and_display(\"A cat sitting on a bench\")\n",
        "plot_losses(train_losses_G, train_losses_D)\n"
      ],
      "metadata": {
        "id": "AxmDjz2HFfd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text_prompt = \"a cat under a tree\"\n",
        "generated_image = inference_pipeline(text_prompt)\n",
        "generated_image = generated_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "generated_image = (generated_image + 1) / 2  # Denormalize\n",
        "plt.imshow(generated_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7wo3K2lGFhGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params_gen = count_parameters(generator)\n",
        "total_params_dis = count_parameters(discriminator)\n",
        "total_params = total_params_gen + total_params_dis\n",
        "print(f\"Total number of parameters in the model: {total_params}\")\n"
      ],
      "metadata": {
        "id": "s-hXiusNFiiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}